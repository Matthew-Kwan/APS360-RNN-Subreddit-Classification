{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jMCNtgNCtnN_"
   },
   "source": [
    "# APS360 - Classifying Subreddits\n",
    "\n",
    "Bassam Bibi<br>\n",
    "Matthew Kwan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "njrikOnetnOE"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YG1vq0LetnOI"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchtext\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nrZg60TetnPU"
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wsu192f_tnP0",
    "outputId": "ae1735f0-edeb-4654-9cda-9dc0ad3a5800"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>titles</th>\n",
       "      <th>post</th>\n",
       "      <th>score</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>'New \"Discovery Mode\" turns video game \"Assass...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53786</td>\n",
       "      <td>https://www.theverge.com/2018/2/20/17033024/as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>'We are not here to help you with your End of ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38419</td>\n",
       "      <td>https://www.reddit.com/r/history/comments/8pw3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>\"A 1776 excerpt from John Adam's diary where h...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35984</td>\n",
       "      <td>https://founders.archives.gov/documents/Adams/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>'Famous Viking warrior burial revealed to be t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34919</td>\n",
       "      <td>http://www.news.com.au/technology/science/arch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>\"3,000-year-old underwater castle discovered i...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34196</td>\n",
       "      <td>https://inhabitat.com/3000-year-old-underwater...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  id  subreddit  \\\n",
       "0           0   1          0   \n",
       "1           1   2          0   \n",
       "2           2   3          0   \n",
       "3           3   4          0   \n",
       "4           4   5          0   \n",
       "\n",
       "                                              titles post  score  \\\n",
       "0  'New \"Discovery Mode\" turns video game \"Assass...  NaN  53786   \n",
       "1  'We are not here to help you with your End of ...  NaN  38419   \n",
       "2  \"A 1776 excerpt from John Adam's diary where h...  NaN  35984   \n",
       "3  'Famous Viking warrior burial revealed to be t...  NaN  34919   \n",
       "4  \"3,000-year-old underwater castle discovered i...  NaN  34196   \n",
       "\n",
       "                                                 url  \n",
       "0  https://www.theverge.com/2018/2/20/17033024/as...  \n",
       "1  https://www.reddit.com/r/history/comments/8pw3...  \n",
       "2  https://founders.archives.gov/documents/Adams/...  \n",
       "3  http://www.news.com.au/technology/science/arch...  \n",
       "4  https://inhabitat.com/3000-year-old-underwater...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./data/removed_b_datav4.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2FRlbeFHtnQs"
   },
   "outputs": [],
   "source": [
    "def split_post(post):\n",
    "    # separate punctuations\n",
    "    post = post.replace(\".\", \" . \") \\\n",
    "                 .replace(\",\", \" , \") \\\n",
    "                 .replace(\";\", \" ; \") \\\n",
    "                 .replace(\"?\", \" ? \") \\\n",
    "\n",
    "    return post.split() # Returns each word as an index in a list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WZLk85PctnQ6",
    "outputId": "6aa71d7b-1058-4564-8f59-37a7a72a2cdb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache\\glove.6B.zip:   2%|â–Š                                                  | 14.6M/862M [00:18<27:48, 508kB/s]"
     ]
    }
   ],
   "source": [
    "# Download pre-trained glove vectors\n",
    "\n",
    "glove = torchtext.vocab.GloVe(name=\"6B\", dim=50, max_vectors=10000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7ejcBX5ktnRf"
   },
   "outputs": [],
   "source": [
    "def get_post_words(glove_vector):\n",
    "    train, valid, test = [], [], []\n",
    "    for i, line in enumerate(df.post):\n",
    "        if i%3 == 0:\n",
    "            if line is np.nan:\n",
    "                post = df.titles[i]\n",
    "            else:\n",
    "                post = line\n",
    "            \n",
    "            idxs = [glove_vector.stoi[w]        # lookup the index of word\n",
    "                    for w in split_post(post)\n",
    "                    if w in glove_vector.stoi] # keep words that has an embedding\n",
    "            if not idxs: # ignore posts without any word with an embedding\n",
    "                continue\n",
    "            idxs = torch.tensor(idxs) # convert list to pytorch tensor\n",
    "            label = torch.tensor(df.subreddit[i]).long() #IMPORTANT, need to convert this to numerical category\n",
    "            if i % 5 < 3:\n",
    "                train.append((idxs, label))\n",
    "            elif i % 5 == 4:\n",
    "                valid.append((idxs, label))\n",
    "            else:\n",
    "                test.append((idxs, label))\n",
    "    return train, valid, test\n",
    "\n",
    "train,valid,test = get_post_words(glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train Length: {} Val Length: {} Test Length: {}\".format(len(train),len(valid),len(test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_DeNHd8oB5Et"
   },
   "source": [
    "## Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BL5k3mqWB8eO"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class PostBatcher:\n",
    "    def __init__(self, posts, batch_size=32, drop_last=False):\n",
    "        # store posts by length\n",
    "        self.posts_by_length = {}\n",
    "        for words, label in posts:\n",
    "            # compute the length of the post\n",
    "            wlen = words.shape[0]\n",
    "            # put the posts in the correct key inside self.posts_by_length\n",
    "            if wlen not in self.posts_by_length:\n",
    "                self.posts_by_length[wlen] = []\n",
    "            self.posts_by_length[wlen].append((words, label),)\n",
    "         \n",
    "        #  create a DataLoader for each set of posts of the same length\n",
    "        self.loaders = {wlen : torch.utils.data.DataLoader(\n",
    "                                    posts,\n",
    "                                    batch_size=batch_size,\n",
    "                                    shuffle=True,\n",
    "                                    drop_last=drop_last) # omit last batch if smaller than batch_size\n",
    "            for wlen, posts in self.posts_by_length.items()}\n",
    "        \n",
    "    def __iter__(self): # called by Python to create an iterator\n",
    "        # make an iterator for every post length\n",
    "        iters = [iter(loader) for loader in self.loaders.values()]\n",
    "        while iters:\n",
    "            # pick an iterator (a length)\n",
    "            im = random.choice(iters)\n",
    "            try:\n",
    "                yield next(im)\n",
    "            except StopIteration:\n",
    "                # no more elements in the iterator, remove it\n",
    "                iters.remove(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = PostBatcher(train,batch_size=16,drop_last=True)\n",
    "valid_loader = PostBatcher(train,batch_size=16,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (posts, labels) in enumerate(train_loader):\n",
    "    if i > 5: break\n",
    "    print(posts.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s2w9-_PmtnR6"
   },
   "source": [
    "## Preliminary Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "apGIcGa5tnSI"
   },
   "outputs": [],
   "source": [
    "class RedditLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes=13):\n",
    "        super(RedditLSTM, self).__init__()\n",
    "        self.emb = nn.Embedding.from_pretrained(glove.vectors)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.LSTM(input_size, hidden_size, batch_first=True, num_layers=3)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Look up the embedding\n",
    "        x = self.emb(x)\n",
    "        # Set an initial hidden state and cell state\n",
    "        h0 = torch.zeros(3, x.size(0), self.hidden_size)\n",
    "        c0 = torch.zeros(3, x.size(0), self.hidden_size)\n",
    "        # Forward propagate the LSTM\n",
    "        out, _ = self.rnn(x, (h0, c0))\n",
    "        # Pass the output of the last time step to the classifier\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "960Ndm61tnSU"
   },
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q_2QTUC1tnSd"
   },
   "outputs": [],
   "source": [
    "# Function to calculate the accuracy of the results. \n",
    "# Returns: # of Correct Predictions / Total # of Predictions\n",
    "\n",
    "def get_accuracy(model, data_loader, find_loss=False):\n",
    "  \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    correct, total = 0, 0\n",
    "    for posts, labels in data_loader:\n",
    "        output = model(posts)\n",
    "        if find_loss == True:\n",
    "            loss = criterion(output,labels)\n",
    "        pred = output.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "        total += labels.shape[0]\n",
    "   \n",
    "    if find_loss == False:\n",
    "        return correct / total\n",
    "    else:\n",
    "        return correct / total,loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9Ws5mNrbtnTZ"
   },
   "outputs": [],
   "source": [
    "def train_rnn_network(model, train, valid, num_epochs=200, learning_rate=1e-4):\n",
    "    criterion = nn.CrossEntropyLoss() # Need Cross Entropy for Multi-Classification\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    losses, train_acc, valid_acc, val_losses = [], [], [], []\n",
    "    epochs = []\n",
    "    for epoch in range(num_epochs):\n",
    "        for posts, labels in train:\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(posts)\n",
    "            loss = criterion(pred, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        losses.append(float(loss))\n",
    "\n",
    "        epochs.append(epoch)\n",
    "        train_acc.append(get_accuracy(model, train_loader))\n",
    "        val_acc,val_loss = get_accuracy(model, valid_loader, find_loss=True)\n",
    "        val_losses.append(val_loss)\n",
    "        valid_acc.append(val_acc)\n",
    "        if epoch%50 == 0:\n",
    "            print(\"Epoch %d; Train Loss %f; Validation Loss %f; Train Acc %f; Val Acc %f\" % (\n",
    "                  epoch, loss, val_loss, train_acc[-1], valid_acc[-1]))\n",
    "    # plotting\n",
    "    plt.title(\"Training Curve\")\n",
    "    plt.plot(losses, label=\"Train\")\n",
    "    plt.plot(val_losses, label=\"Validation\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.title(\"Training Curve\")\n",
    "    plt.plot(epochs, train_acc, label=\"Train\")\n",
    "    plt.plot(epochs, valid_acc, label=\"Validation\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RedditLSTM(50,50)\n",
    "train_rnn_network(model,train_loader,valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Test Loader\n",
    "test_loader = PostBatcher(test,batch_size=16,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our accuracy on our test set was: 0.4041666666666667\n"
     ]
    }
   ],
   "source": [
    "test_acc = get_accuracy(model,test_loader)\n",
    "print(\"Our accuracy on our test set was: {}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Model.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
