{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jMCNtgNCtnN_"
   },
   "source": [
    "# APS360 - Classifying Subreddits\n",
    "\n",
    "Bassam Bibi<br>\n",
    "Matthew Kwan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "njrikOnetnOE"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YG1vq0LetnOI"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchtext\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nrZg60TetnPU"
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wsu192f_tnP0",
    "outputId": "ae1735f0-edeb-4654-9cda-9dc0ad3a5800"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>post</th>\n",
       "      <th>score</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>subreddit_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53782</td>\n",
       "      <td>0</td>\n",
       "      <td>New \"Discovery Mode\" turns video game \"Assassi...</td>\n",
       "      <td>https://www.theverge.com/2018/2/20/17033024/as...</td>\n",
       "      <td>history</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Hi everyone and especially our students,\\r\\r\\r...</td>\n",
       "      <td>38426</td>\n",
       "      <td>0</td>\n",
       "      <td>We are not here to help you with your End of T...</td>\n",
       "      <td>https://www.reddit.com/r/history/comments/8pw3...</td>\n",
       "      <td>history</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35982</td>\n",
       "      <td>0</td>\n",
       "      <td>A 1776 excerpt from John Adam's diary where he...</td>\n",
       "      <td>https://founders.archives.gov/documents/Adams/...</td>\n",
       "      <td>history</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34908</td>\n",
       "      <td>0</td>\n",
       "      <td>Famous Viking warrior burial revealed to be th...</td>\n",
       "      <td>http://www.news.com.au/technology/science/arch...</td>\n",
       "      <td>history</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34197</td>\n",
       "      <td>0</td>\n",
       "      <td>3,000-year-old underwater castle discovered in...</td>\n",
       "      <td>https://inhabitat.com/3000-year-old-underwater...</td>\n",
       "      <td>history</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                               post  score  subreddit  \\\n",
       "0   0                                                NaN  53782          0   \n",
       "1   1  Hi everyone and especially our students,\\r\\r\\r...  38426          0   \n",
       "2   2                                                NaN  35982          0   \n",
       "3   3                                                NaN  34908          0   \n",
       "4   4                                                NaN  34197          0   \n",
       "\n",
       "                                               title  \\\n",
       "0  New \"Discovery Mode\" turns video game \"Assassi...   \n",
       "1  We are not here to help you with your End of T...   \n",
       "2  A 1776 excerpt from John Adam's diary where he...   \n",
       "3  Famous Viking warrior burial revealed to be th...   \n",
       "4  3,000-year-old underwater castle discovered in...   \n",
       "\n",
       "                                                 url subreddit_name  \n",
       "0  https://www.theverge.com/2018/2/20/17033024/as...        history  \n",
       "1  https://www.reddit.com/r/history/comments/8pw3...        history  \n",
       "2  https://founders.archives.gov/documents/Adams/...        history  \n",
       "3  http://www.news.com.au/technology/science/arch...        history  \n",
       "4  https://inhabitat.com/3000-year-old-underwater...        history  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./data/updated_datav4.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Function for Punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2FRlbeFHtnQs"
   },
   "outputs": [],
   "source": [
    "def split_post(post):\n",
    "    # separate punctuations\n",
    "    post = post.replace(\".\", \" . \") \\\n",
    "                 .replace(\",\", \" , \") \\\n",
    "                 .replace(\";\", \" ; \") \\\n",
    "                 .replace(\"?\", \" ? \") \\\n",
    "                 .replace(\"\\\"\", \" \\\" \") \\\n",
    "                 .replace(\"!\", \" ! \") \\\n",
    "                 .replace(\":\", \" : \")\n",
    "\n",
    "    post = post.split() # Returns each word as an index in a list\n",
    "    \n",
    "    # Define remove list:\n",
    "    rmv = ['that', 'what', 'at', 'just', 'you', 'up', 'in', 'and', 'my', 'for', 'from', 'how',\n",
    "           'but', 'a', 'as', 'to', 'about', 'when', 'is', 'be', 'are', 'out', 'not', 'can', 'so',\n",
    "           'this', 'do', 'it', 'an', 'like', 'would', 'time', 'with', 'if', 'know', 'of', 'I', 'the',\n",
    "           'all', 'have', 'had', 'me', 'one', 'will', 'was', \"I'm\", 'them', 'they', 'there', 'on']\n",
    "    \n",
    "    post = [x for x in post if x not in rmv]\n",
    "    \n",
    "    return post"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download GloVe Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WZLk85PctnQ6",
    "outputId": "6aa71d7b-1058-4564-8f59-37a7a72a2cdb"
   },
   "outputs": [],
   "source": [
    "# Download pre-trained glove vectors\n",
    "\n",
    "glove = torchtext.vocab.GloVe(name=\"6B\", dim=100, max_vectors=10000) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function the Split Data into Train, Test and Valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7ejcBX5ktnRf"
   },
   "outputs": [],
   "source": [
    "def get_post_words(glove_vector):\n",
    "    train, valid, test = [], [], []\n",
    "    for i, line in enumerate(df.post):\n",
    "        # If there is no text content in the post, then use the title of the post instead\n",
    "            \n",
    "        if line is np.nan:\n",
    "            post = df.title[i]\n",
    "        else:\n",
    "            #  Truncate down to 300 characters\n",
    "            post = line[:300]\n",
    "           \n",
    "        idxs = [glove_vector.stoi[w]        # lookup the index of word\n",
    "                for w in split_post(post)\n",
    "                if w in glove_vector.stoi] # keep words that has an embedding\n",
    "        if not idxs: # ignore posts without any word with an embedding\n",
    "            continue\n",
    "        idxs = torch.tensor(idxs) # convert list to pytorch tensor\n",
    "        label = torch.tensor(df.subreddit[i]).long() #IMPORTANT, need to convert this to numerical category\n",
    "        if i % 5 < 3:\n",
    "            train.append((idxs, label))\n",
    "        elif i % 5 == 4:\n",
    "            valid.append((idxs, label))\n",
    "        else:\n",
    "            test.append((idxs, label))\n",
    "    return train, valid, test\n",
    "\n",
    "train,valid,test = get_post_words(glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Length: 7434 Val Length: 2488 Test Length: 2486\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Length: {} Val Length: {} Test Length: {}\".format(len(train),len(valid),len(test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_DeNHd8oB5Et"
   },
   "source": [
    "## Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BL5k3mqWB8eO"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class PostBatcher:\n",
    "    def __init__(self, posts, batch_size=32, drop_last=False):\n",
    "        # store posts by length\n",
    "        self.posts_by_length = {}\n",
    "        for words, label in posts:\n",
    "            # compute the length of the post\n",
    "            wlen = words.shape[0]\n",
    "            # put the posts in the correct key inside self.posts_by_length\n",
    "            if wlen not in self.posts_by_length:\n",
    "                self.posts_by_length[wlen] = []\n",
    "            self.posts_by_length[wlen].append((words, label),)\n",
    "         \n",
    "        #  create a DataLoader for each set of posts of the same length\n",
    "        self.loaders = {wlen : torch.utils.data.DataLoader(\n",
    "                                    posts,\n",
    "                                    batch_size=batch_size,\n",
    "                                    shuffle=True,\n",
    "                                    drop_last=drop_last) # omit last batch if smaller than batch_size\n",
    "            for wlen, posts in self.posts_by_length.items()}\n",
    "        \n",
    "    def __iter__(self): # called by Python to create an iterator\n",
    "        # make an iterator for every post length\n",
    "        iters = [iter(loader) for loader in self.loaders.values()]\n",
    "        while iters:\n",
    "            # pick an iterator (a length)\n",
    "            im = random.choice(iters)\n",
    "            try:\n",
    "                yield next(im)\n",
    "            except StopIteration:\n",
    "                # no more elements in the iterator, remove it\n",
    "                iters.remove(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our batching function working, let's define our data loaders. Let's start with a batch-size of 16:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = PostBatcher(train,batch_size=128,drop_last=True)\n",
    "valid_loader = PostBatcher(train,batch_size=128,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 27]) torch.Size([128])\n",
      "torch.Size([128, 11]) torch.Size([128])\n",
      "torch.Size([128, 8]) torch.Size([128])\n",
      "torch.Size([128, 20]) torch.Size([128])\n",
      "torch.Size([128, 10]) torch.Size([128])\n",
      "torch.Size([128, 5]) torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "for i, (posts, labels) in enumerate(train_loader):\n",
    "    if i > 5: break\n",
    "    print(posts.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.PostBatcher at 0x2cf848279b0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s2w9-_PmtnR6"
   },
   "source": [
    "## Preliminary Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "apGIcGa5tnSI"
   },
   "outputs": [],
   "source": [
    "class RedditLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes=13):\n",
    "        super(RedditLSTM, self).__init__()\n",
    "        self.emb = nn.Embedding.from_pretrained(glove.vectors)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.rnn = nn.LSTM(input_size, hidden_size, batch_first=True, num_layers=3)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Look up the embedding\n",
    "        x = self.emb(x)\n",
    "        # Apply dropout\n",
    "        x = self.dropout(x)\n",
    "        # Set an initial hidden state and cell state\n",
    "        h0 = torch.zeros(3, x.size(0), self.hidden_size)\n",
    "        c0 = torch.zeros(3, x.size(0), self.hidden_size)\n",
    "        # Forward propagate the LSTM\n",
    "        out, _ = self.rnn(x, (h0, c0))\n",
    "        # Pass the output of the last time step to the classifier\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "960Ndm61tnSU"
   },
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q_2QTUC1tnSd"
   },
   "outputs": [],
   "source": [
    "# Function to calculate the accuracy of the results. \n",
    "# Returns: # of Correct Predictions / Total # of Predictions\n",
    "\n",
    "def get_accuracy(model, data_loader, find_loss=False):\n",
    "  \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    correct, total, loss = 0, 0, 0\n",
    "    for posts, labels in data_loader:\n",
    "        output = model(posts)\n",
    "        if find_loss == True:\n",
    "            loss = criterion(output,labels)\n",
    "        pred = output.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "        total += labels.shape[0]\n",
    "   \n",
    "    if find_loss == False:\n",
    "        return correct / (total+0.0000001)\n",
    "    else:\n",
    "        return correct / (total+0.0000001),loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9Ws5mNrbtnTZ"
   },
   "outputs": [],
   "source": [
    "def train_rnn_network(model, train, valid, batch_size=32, num_epochs=200, learning_rate=1e-4):\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss() # Need Cross Entropy for Multi-Classification\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    train_loader = PostBatcher(train,batch_size=batch_size,drop_last=True)\n",
    "    valid_loader = PostBatcher(valid,batch_size=batch_size,drop_last=True)\n",
    "    \n",
    "    losses, train_acc, valid_acc, val_losses = [], [], [], []\n",
    "    loss = 0\n",
    "    epochs = []\n",
    "    for epoch in range(num_epochs):\n",
    "        for posts, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(posts)\n",
    "            loss = criterion(pred, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        losses.append(float(loss))\n",
    "\n",
    "        epochs.append(epoch)\n",
    "        train_acc.append(get_accuracy(model, train_loader))\n",
    "        val_acc,val_loss = get_accuracy(model, valid_loader, find_loss=True)\n",
    "        val_losses.append(val_loss)\n",
    "        valid_acc.append(val_acc)\n",
    "        if epoch %10 == 0:\n",
    "            \n",
    "            print(\"Epoch %d; Train Loss %f; Validation Loss %f; Train Acc %f; Val Acc %f\" % ( \\\n",
    "                  epoch, loss, val_loss, train_acc[-1], valid_acc[-1]))\n",
    "            \n",
    "        # checkpoints\n",
    "        if (epoch) % 5== 0:\n",
    "            torch.save(model.state_dict(), \"model_lr{}_epoch{}_bs{}_adam\".format(learning_rate,epoch,batch_size))\n",
    "    \n",
    "    # plotting\n",
    "    plt.title(\"Training Curve\")\n",
    "    plt.plot(losses, label=\"Train\")\n",
    "    plt.plot(val_losses, label=\"Validation\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.title(\"Training Curve\")\n",
    "    plt.plot(epochs, train_acc, label=\"Train\")\n",
    "    plt.plot(epochs, valid_acc, label=\"Validation\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "    \n",
    "    return loss, val_loss, train_acc[-1], valid_acc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rnn_network_rms(model, train, valid, batch_size=32, num_epochs=200, learning_rate=0.01):\n",
    "    criterion = nn.CrossEntropyLoss() # Need Cross Entropy for Multi-Classification\n",
    "    optimizer = torch.optim.RMSprop(model.parameters(), lr = learning_rate)\n",
    "    train_loader = PostBatcher(train,batch_size=batch_size,drop_last=True)\n",
    "    valid_loader = PostBatcher(valid,batch_size=batch_size,drop_last=True)\n",
    "    losses, train_acc, valid_acc, val_losses = [], [], [], []\n",
    "    loss = 0\n",
    "    epochs = []\n",
    "    for epoch in range(num_epochs):\n",
    "        for posts, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(posts)\n",
    "            loss = criterion(pred, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        losses.append(float(loss))\n",
    "\n",
    "        epochs.append(epoch)\n",
    "        train_acc.append(get_accuracy(model, train_loader))\n",
    "        val_acc,val_loss = get_accuracy(model, valid_loader, find_loss=True)\n",
    "        val_losses.append(val_loss)\n",
    "        valid_acc.append(val_acc)\n",
    "        if epoch%10 == 0:\n",
    "            print(\"Epoch %d; Train Loss %f; Validation Loss %f; Train Acc %f; Val Acc %f\" % (\n",
    "                  epoch, loss, val_loss, train_acc[-1], valid_acc[-1]))\n",
    "        \n",
    "        # checkpoints\n",
    "        if (epoch) % 5== 0:\n",
    "            torch.save(model.state_dict(), \"model_lr{}_epoch{}_bs{}_rms\".format(learning_rate,epoch,batch_size))\n",
    "            \n",
    "    # plotting\n",
    "    plt.title(\"Training Curve\")\n",
    "    plt.plot(losses, label=\"Train\")\n",
    "    plt.plot(val_losses, label=\"Validation\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.title(\"Training Curve\")\n",
    "    plt.plot(epochs, train_acc, label=\"Train\")\n",
    "    plt.plot(epochs, valid_acc, label=\"Validation\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "    \n",
    "    return loss, val_loss, train_acc[-1], valid_acc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rnn_network_ada(model, train, valid, batch_size=32, num_epochs=200, learning_rate=0.01):\n",
    "    criterion = nn.CrossEntropyLoss() # Need Cross Entropy for Multi-Classification\n",
    "    optimizer = torch.optim.Adagrad(model.parameters(), lr = learning_rate)\n",
    "    train_loader = PostBatcher(train,batch_size=batch_size,drop_last=True)\n",
    "    valid_loader = PostBatcher(valid,batch_size=batch_size,drop_last=True)\n",
    "    losses, train_acc, valid_acc, val_losses = [], [], [], []\n",
    "    loss = 0\n",
    "    epochs = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for posts, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(posts)\n",
    "            loss = criterion(pred, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        losses.append(float(loss))\n",
    "\n",
    "        epochs.append(epoch)\n",
    "        train_acc.append(get_accuracy(model, train_loader))\n",
    "        val_acc,val_loss = get_accuracy(model, valid_loader, find_loss=True)\n",
    "        val_losses.append(val_loss)\n",
    "        valid_acc.append(val_acc)\n",
    "        if epoch%10 == 0:\n",
    "            print(\"Epoch %d; Train Loss %f; Validation Loss %f; Train Acc %f; Val Acc %f\" % (\n",
    "                  epoch, loss, val_loss, train_acc[-1], valid_acc[-1]))\n",
    "        \n",
    "        # checkpoints\n",
    "        if (epoch) % 5== 0:\n",
    "            torch.save(model.state_dict(), \"model_lr{}_epoch{}_bs{}_ada\".format(learning_rate,epoch,batch_size))\n",
    "            \n",
    "    # plotting\n",
    "    plt.title(\"Training Curve\")\n",
    "    plt.plot(losses, label=\"Train\")\n",
    "    plt.plot(val_losses, label=\"Validation\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.title(\"Training Curve\")\n",
    "    plt.plot(epochs, train_acc, label=\"Train\")\n",
    "    plt.plot(epochs, valid_acc, label=\"Validation\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "    \n",
    "    return loss, val_loss, train_acc[-1], valid_acc[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Search\n",
    "A function to help with hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RandomSearch(n_iters, batch_sizes, learning_rates, hidden_sizes, opt=\"adam\"):\n",
    "    \"\"\"\n",
    "    n_iters: number of iterations to perform.\n",
    "    batch_sizes: list of batch sizes.\n",
    "    learning_rates: list of learning rates.\n",
    "    hidden_sizes: list of hidden unit sizes.\n",
    "    \"\"\"\n",
    "    list_data = []\n",
    "    \n",
    "    for i in range(n_iters):\n",
    "        bs = np.random.randint(0,high=len(batch_sizes))\n",
    "        lr = np.random.randint(0,high=len(learning_rates))\n",
    "        hs = np.random.randint(0,high=len(hidden_sizes))\n",
    "        \n",
    "        model = RedditLSTM(100,hidden_sizes[hs])\n",
    "        \n",
    "        print(\"Batch Size: {} Learning Rate: {} Hidden Size: {}\".format(batch_sizes[bs],learning_rates[lr],\n",
    "                                                                        hidden_sizes[hs]))\n",
    "        \n",
    "        if opt == \"ada\":\n",
    "            print(\"ada\")\n",
    "            result = train_rnn_network_ada(model,train,valid, learning_rate=learning_rates[lr], batch_size=batch_sizes[bs],\n",
    "                                           num_epochs=100)\n",
    "        elif opt == \"RMS\":\n",
    "            print(\"RMS\")\n",
    "            result = train_rnn_network_rms(model,train,valid,learning_rate=learning_rates[lr],batch_size=batch_sizes[bs],num_epochs=100)\n",
    "        else:\n",
    "            result = train_rnn_network(model,train,valid,learning_rate=learning_rates[lr],batch_size=batch_sizes[bs],num_epochs=100)\n",
    "        \n",
    "        test_result = get_accuracy(model,test_loader)\n",
    "        print(\"Test accuracy: {}\".format(test_result))\n",
    "        list_data.append([i,batch_sizes[bs],learning_rates[lr],hidden_sizes[hs],result[0],result[1],\n",
    "                          result[2],result[3],test_result])\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    df = pd.DataFrame(list_data,columns=[\"Iter\", \"batch size\", \"learning rate\", \"hidden size\", \"train loss\",\n",
    "                                         \"valid loss\",\"train acc\", \"valid acc\",\"test_acc\"])\n",
    "    \n",
    "    file_name = \"randomsearch_\"+opt+\".csv\"\n",
    "    \n",
    "    df.to_csv(file_name,index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size: 16 Learning Rate: 0.005 Hidden Size: 32\n",
      "RMS\n"
     ]
    }
   ],
   "source": [
    "batch_sizes = [8,16,32,64,128]\n",
    "learning_rates = [0.01,0.005,0.001]\n",
    "hidden_sizes = [32,64,128,256]\n",
    "\n",
    "RandomSearch(6,batch_sizes,learning_rates,hidden_sizes,opt=\"RMS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-bs:8, lr:1e-05, hs:128\n",
    "Epoch 0; Train Loss 2.542450; Validation Loss 2.514309; Train Acc 0.099684; Val Acc 0.105556\n",
    "\n",
    "-bs:16, lr:5e-06, hs:64\n",
    "Epoch 0; Train Loss 2.589120; Validation Loss 2.601496; Train Acc 0.065045; Val Acc 0.043883\n",
    "Epoch 50; Train Loss 2.131216; Validation Loss 2.020390; Train Acc 0.180430; Val Acc 0.200798           \n",
    "\n",
    "-bs:16, lr:0.0005, hs:128\n",
    "Epoch 0; Train Loss 1.612568; Validation Loss 1.933496; Train Acc 0.219457; Val Acc 0.271277\n",
    "Epoch 50; Train Loss 0.113162; Validation Loss 0.418212; Train Acc 0.964649; Val Acc 0.555851\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Model.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
