{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jMCNtgNCtnN_"
   },
   "source": [
    "# APS360 - Classifying Subreddits\n",
    "\n",
    "Bassam Bibi<br>\n",
    "Matthew Kwan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "njrikOnetnOE"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YG1vq0LetnOI"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchtext\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nrZg60TetnPU"
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wsu192f_tnP0",
    "outputId": "ae1735f0-edeb-4654-9cda-9dc0ad3a5800"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>post</th>\n",
       "      <th>score</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>subreddit_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53782</td>\n",
       "      <td>0</td>\n",
       "      <td>New \"Discovery Mode\" turns video game \"Assassi...</td>\n",
       "      <td>https://www.theverge.com/2018/2/20/17033024/as...</td>\n",
       "      <td>history</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Hi everyone and especially our students,\\r\\r\\r...</td>\n",
       "      <td>38426</td>\n",
       "      <td>0</td>\n",
       "      <td>We are not here to help you with your End of T...</td>\n",
       "      <td>https://www.reddit.com/r/history/comments/8pw3...</td>\n",
       "      <td>history</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35982</td>\n",
       "      <td>0</td>\n",
       "      <td>A 1776 excerpt from John Adam's diary where he...</td>\n",
       "      <td>https://founders.archives.gov/documents/Adams/...</td>\n",
       "      <td>history</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34908</td>\n",
       "      <td>0</td>\n",
       "      <td>Famous Viking warrior burial revealed to be th...</td>\n",
       "      <td>http://www.news.com.au/technology/science/arch...</td>\n",
       "      <td>history</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34197</td>\n",
       "      <td>0</td>\n",
       "      <td>3,000-year-old underwater castle discovered in...</td>\n",
       "      <td>https://inhabitat.com/3000-year-old-underwater...</td>\n",
       "      <td>history</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                               post  score  subreddit  \\\n",
       "0   0                                                NaN  53782          0   \n",
       "1   1  Hi everyone and especially our students,\\r\\r\\r...  38426          0   \n",
       "2   2                                                NaN  35982          0   \n",
       "3   3                                                NaN  34908          0   \n",
       "4   4                                                NaN  34197          0   \n",
       "\n",
       "                                               title  \\\n",
       "0  New \"Discovery Mode\" turns video game \"Assassi...   \n",
       "1  We are not here to help you with your End of T...   \n",
       "2  A 1776 excerpt from John Adam's diary where he...   \n",
       "3  Famous Viking warrior burial revealed to be th...   \n",
       "4  3,000-year-old underwater castle discovered in...   \n",
       "\n",
       "                                                 url subreddit_name  \n",
       "0  https://www.theverge.com/2018/2/20/17033024/as...        history  \n",
       "1  https://www.reddit.com/r/history/comments/8pw3...        history  \n",
       "2  https://founders.archives.gov/documents/Adams/...        history  \n",
       "3  http://www.news.com.au/technology/science/arch...        history  \n",
       "4  https://inhabitat.com/3000-year-old-underwater...        history  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./data/updated_datav4.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Function for Punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2FRlbeFHtnQs"
   },
   "outputs": [],
   "source": [
    "def split_post(post):\n",
    "    # separate punctuations\n",
    "    post = post.replace(\".\", \" . \") \\\n",
    "                 .replace(\",\", \" , \") \\\n",
    "                 .replace(\";\", \" ; \") \\\n",
    "                 .replace(\"?\", \" ? \") \n",
    "\n",
    "    return post.split() # Returns each word as an index in a list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download GloVe Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WZLk85PctnQ6",
    "outputId": "6aa71d7b-1058-4564-8f59-37a7a72a2cdb"
   },
   "outputs": [],
   "source": [
    "# Download pre-trained glove vectors\n",
    "\n",
    "glove = torchtext.vocab.GloVe(name=\"6B\", dim=50, max_vectors=10000) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function the Split Data into Train, Test and Valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7ejcBX5ktnRf"
   },
   "outputs": [],
   "source": [
    "def get_post_words(glove_vector):\n",
    "    train, valid, test = [], [], []\n",
    "    for i, line in enumerate(df.post):\n",
    "        if i%3 == 0:\n",
    "            \n",
    "            # If there is no text content in the post, then use the title of the post instead\n",
    "            \n",
    "            if line is np.nan:\n",
    "                post = df.title[i]\n",
    "            else:\n",
    "                post = line\n",
    "            \n",
    "            idxs = [glove_vector.stoi[w]        # lookup the index of word\n",
    "                    for w in split_post(post)\n",
    "                    if w in glove_vector.stoi] # keep words that has an embedding\n",
    "            if not idxs: # ignore posts without any word with an embedding\n",
    "                continue\n",
    "            idxs = torch.tensor(idxs) # convert list to pytorch tensor\n",
    "            label = torch.tensor(df.subreddit[i]).long() #IMPORTANT, need to convert this to numerical category\n",
    "            if i % 5 < 3:\n",
    "                train.append((idxs, label))\n",
    "            elif i % 5 == 4:\n",
    "                valid.append((idxs, label))\n",
    "            else:\n",
    "                test.append((idxs, label))\n",
    "    return train, valid, test\n",
    "\n",
    "train,valid,test = get_post_words(glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Length: 2497 Val Length: 836 Test Length: 833\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Length: {} Val Length: {} Test Length: {}\".format(len(train),len(valid),len(test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_DeNHd8oB5Et"
   },
   "source": [
    "## Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BL5k3mqWB8eO"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class PostBatcher:\n",
    "    def __init__(self, posts, batch_size=32, drop_last=False):\n",
    "        # store posts by length\n",
    "        self.posts_by_length = {}\n",
    "        for words, label in posts:\n",
    "            # compute the length of the post\n",
    "            wlen = words.shape[0]\n",
    "            # put the posts in the correct key inside self.posts_by_length\n",
    "            if wlen not in self.posts_by_length:\n",
    "                self.posts_by_length[wlen] = []\n",
    "            self.posts_by_length[wlen].append((words, label),)\n",
    "         \n",
    "        #  create a DataLoader for each set of posts of the same length\n",
    "        self.loaders = {wlen : torch.utils.data.DataLoader(\n",
    "                                    posts,\n",
    "                                    batch_size=batch_size,\n",
    "                                    shuffle=True,\n",
    "                                    drop_last=drop_last) # omit last batch if smaller than batch_size\n",
    "            for wlen, posts in self.posts_by_length.items()}\n",
    "        \n",
    "    def __iter__(self): # called by Python to create an iterator\n",
    "        # make an iterator for every post length\n",
    "        iters = [iter(loader) for loader in self.loaders.values()]\n",
    "        while iters:\n",
    "            # pick an iterator (a length)\n",
    "            im = random.choice(iters)\n",
    "            try:\n",
    "                yield next(im)\n",
    "            except StopIteration:\n",
    "                # no more elements in the iterator, remove it\n",
    "                iters.remove(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our batching function working, let's define our data loaders. Let's start with a batch-size of 16:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = PostBatcher(train,batch_size=16,drop_last=True)\n",
    "valid_loader = PostBatcher(valid,batch_size=16,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 19]) torch.Size([16])\n",
      "torch.Size([16, 8]) torch.Size([16])\n",
      "torch.Size([16, 2]) torch.Size([16])\n",
      "torch.Size([16, 27]) torch.Size([16])\n",
      "torch.Size([16, 9]) torch.Size([16])\n",
      "torch.Size([16, 3]) torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "for i, (posts, labels) in enumerate(train_loader):\n",
    "    if i > 5: break\n",
    "    print(posts.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.PostBatcher at 0x261d24ab518>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s2w9-_PmtnR6"
   },
   "source": [
    "## Preliminary Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "apGIcGa5tnSI"
   },
   "outputs": [],
   "source": [
    "class RedditLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes=13):\n",
    "        super(RedditLSTM, self).__init__()\n",
    "        self.emb = nn.Embedding.from_pretrained(glove.vectors)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.LSTM(input_size, hidden_size, batch_first=True, num_layers=3)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Look up the embedding\n",
    "        x = self.emb(x)\n",
    "        # Set an initial hidden state and cell state\n",
    "        h0 = torch.zeros(3, x.size(0), self.hidden_size)\n",
    "        c0 = torch.zeros(3, x.size(0), self.hidden_size)\n",
    "        # Forward propagate the LSTM\n",
    "        out, _ = self.rnn(x, (h0, c0))\n",
    "        # Pass the output of the last time step to the classifier\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "960Ndm61tnSU"
   },
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q_2QTUC1tnSd"
   },
   "outputs": [],
   "source": [
    "# Function to calculate the accuracy of the results. \n",
    "# Returns: # of Correct Predictions / Total # of Predictions\n",
    "\n",
    "def get_accuracy(model, data_loader, find_loss=False):\n",
    "  \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    correct, total = 0, 0\n",
    "    for posts, labels in data_loader:\n",
    "        output = model(posts)\n",
    "        if find_loss == True:\n",
    "            loss = criterion(output,labels)\n",
    "        pred = output.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "        total += labels.shape[0]\n",
    "   \n",
    "    if find_loss == False:\n",
    "        return correct / total\n",
    "    else:\n",
    "        return correct / total,loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9Ws5mNrbtnTZ"
   },
   "outputs": [],
   "source": [
    "def train_rnn_network(model, train, valid, num_epochs=200, learning_rate=1e-4):\n",
    "    criterion = nn.CrossEntropyLoss() # Need Cross Entropy for Multi-Classification\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    losses, train_acc, valid_acc, val_losses = [], [], [], []\n",
    "    epochs = []\n",
    "    for epoch in range(num_epochs):\n",
    "        for posts, labels in train:\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(posts)\n",
    "            loss = criterion(pred, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        losses.append(float(loss))\n",
    "\n",
    "        epochs.append(epoch)\n",
    "        train_acc.append(get_accuracy(model, train_loader))\n",
    "        val_acc,val_loss = get_accuracy(model, valid_loader, find_loss=True)\n",
    "        val_losses.append(val_loss)\n",
    "        valid_acc.append(val_acc)\n",
    "        if epoch%50 == 0:\n",
    "            print(\"Epoch %d; Train Loss %f; Validation Loss %f; Train Acc %f; Val Acc %f\" % (\n",
    "                  epoch, loss, val_loss, train_acc[-1], valid_acc[-1]))\n",
    "    # plotting\n",
    "    plt.title(\"Training Curve\")\n",
    "    plt.plot(losses, label=\"Train\")\n",
    "    plt.plot(val_losses, label=\"Validation\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.title(\"Training Curve\")\n",
    "    plt.plot(epochs, train_acc, label=\"Train\")\n",
    "    plt.plot(epochs, valid_acc, label=\"Validation\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0; Train Loss 2.510890; Validation Loss 2.440137; Train Acc 0.170213; Val Acc 0.163564\n",
      "Epoch 50; Train Loss 1.680194; Validation Loss 1.483324; Train Acc 0.323138; Val Acc 0.315160\n",
      "Epoch 100; Train Loss 1.483804; Validation Loss 1.992002; Train Acc 0.424202; Val Acc 0.429521\n",
      "Epoch 150; Train Loss 1.444066; Validation Loss 1.347528; Train Acc 0.539894; Val Acc 0.545213\n",
      "Epoch 200; Train Loss 1.643146; Validation Loss 1.256178; Train Acc 0.598404; Val Acc 0.586436\n",
      "Epoch 250; Train Loss 0.437440; Validation Loss 0.604233; Train Acc 0.720745; Val Acc 0.715426\n",
      "Epoch 300; Train Loss 0.576510; Validation Loss 1.306043; Train Acc 0.744681; Val Acc 0.744681\n",
      "Epoch 350; Train Loss 0.749690; Validation Loss 0.306805; Train Acc 0.815160; Val Acc 0.813830\n",
      "Epoch 400; Train Loss 0.392854; Validation Loss 0.585332; Train Acc 0.844415; Val Acc 0.837766\n",
      "Epoch 450; Train Loss 0.326381; Validation Loss 0.209531; Train Acc 0.845745; Val Acc 0.867021\n",
      "Epoch 500; Train Loss 0.351617; Validation Loss 1.289989; Train Acc 0.882979; Val Acc 0.884309\n",
      "Epoch 550; Train Loss 0.075982; Validation Loss 0.028859; Train Acc 0.892287; Val Acc 0.898936\n",
      "Epoch 600; Train Loss 0.080444; Validation Loss 0.440921; Train Acc 0.913564; Val Acc 0.910904\n",
      "Epoch 650; Train Loss 0.114695; Validation Loss 0.161667; Train Acc 0.885638; Val Acc 0.889628\n",
      "Epoch 700; Train Loss 0.278898; Validation Loss 0.035578; Train Acc 0.930851; Val Acc 0.929521\n",
      "Epoch 750; Train Loss 0.089891; Validation Loss 0.055655; Train Acc 0.945479; Val Acc 0.938830\n",
      "Epoch 800; Train Loss 0.012598; Validation Loss 0.023237; Train Acc 0.950798; Val Acc 0.946809\n",
      "Epoch 850; Train Loss 0.058778; Validation Loss 0.030545; Train Acc 0.954787; Val Acc 0.950798\n",
      "Epoch 900; Train Loss 0.072646; Validation Loss 0.013957; Train Acc 0.954787; Val Acc 0.953457\n",
      "Epoch 950; Train Loss 0.016116; Validation Loss 0.008336; Train Acc 0.958777; Val Acc 0.956117\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-46f93a1a4ce6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRedditLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain_rnn_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalid_loader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-34-dbffc3e9a99c>\u001b[0m in \u001b[0;36mtrain_rnn_network\u001b[1;34m(model, train, valid, num_epochs, learning_rate)\u001b[0m\n\u001b[0;32m      9\u001b[0m             \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mposts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         \"\"\"\n\u001b[1;32m--> 102\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = RedditLSTM(50,50)\n",
    "train_rnn_network(model,train_loader,valid_loader,num_epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Test Loader\n",
    "test_loader = PostBatcher(test,batch_size=16,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our accuracy on our test set was: 0.546875\n"
     ]
    }
   ],
   "source": [
    "test_acc = get_accuracy(model,test_loader)\n",
    "print(\"Our accuracy on our test set was: {}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning Hyperparameters\n",
    "First, let's try increasing the batch size to 32:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = PostBatcher(train,batch_size=32,drop_last=True)\n",
    "valid_loader = PostBatcher(train,batch_size=32,drop_last=True)\n",
    "model = RedditLSTM(50,50)\n",
    "train_rnn_network(model,train_loader,valid_loader,num_epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = PostBatcher(train,batch_size=64,drop_last=True)\n",
    "valid_loader = PostBatcher(train,batch_size=64,drop_last=True)\n",
    "model = RedditLSTM(50,50)\n",
    "train_rnn_network(model,train_loader,valid_loader,num_epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = PostBatcher(train,batch_size=64,drop_last=True)\n",
    "valid_loader = PostBatcher(train,batch_size=64,drop_last=True)\n",
    "model = RedditLSTM(50,50)\n",
    "train_rnn_network(model,train_loader,valid_loader,learning_rate = 0.5e-3,num_epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = PostBatcher(train,batch_size=64,drop_last=True)\n",
    "valid_loader = PostBatcher(train,batch_size=64,drop_last=True)\n",
    "model = RedditLSTM(50,126)\n",
    "train_rnn_network(model,train_loader,valid_loader,learning_rate = 0.5e-3,num_epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = PostBatcher(train,batch_size=64,drop_last=True)\n",
    "valid_loader = PostBatcher(train,batch_size=64,drop_last=True)\n",
    "model = RedditLSTM(50,256)\n",
    "train_rnn_network(model,train_loader,valid_loader,learning_rate = 0.5e-3,num_epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Model.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
